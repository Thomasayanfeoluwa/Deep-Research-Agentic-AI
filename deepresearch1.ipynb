{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "e2bbb2dd",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import asyncio\n",
                "import operator\n",
                "import requests\n",
                "from typing import List, Annotated, TypedDict, Union\n",
                "\n",
                "from pydantic import BaseModel, Field\n",
                "from IPython.display import display, Markdown\n",
                "from pprint import pprint\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# LangChain & LangGraph Imports\n",
                "from langchain_groq import ChatGroq\n",
                "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
                "from langchain_core.tools import tool\n",
                "from langgraph.graph import StateGraph, START, END\n",
                "from langgraph.graph.message import add_messages\n",
                "from langgraph.checkpoint.memory import MemorySaver\n",
                "from langchain_community.tools.tavily_search import TavilySearchResults\n",
                "\n",
                "# Load environment variables\n",
                "load_dotenv()\n",
                "\n",
                "# Ensure LangSmith Tracing is on if API key is present\n",
                "if os.getenv(\"LANGCHAIN_API_KEY\"):\n",
                "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
                "    os.environ[\"LANGCHAIN_PROJECT\"] = \"AgenticAI_Workshop\" \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "1a3a57d5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 1. CONFIGURATION & INSTRUCTIONS ---\n",
                "# We preserve your exact instructions as constants for the nodes\n",
                "PLANNER_INSTRUCTIONS = \"You are a helpful research assistant. Given a query, come up with a set of web searches to perform to best answer the query. Output 5 terms to query for.\"\n",
                "\n",
                "SEARCH_INSTRUCTIONS = \"You are a research assistant. Given a search term, you search the web for that term and produce a concise summary of the results. The summary must 2-3 paragraphs and less than 300 words. Capture the main points. Write succintly, no need to have complete sentences or good grammar. This will be consumed by someone synthesizing a report, so it's vital you capture the essence and ignore any fluff. Do not include any additional commentary other than the summary itself.\"\n",
                "\n",
                "WRITER_INSTRUCTIONS = (\n",
                "    \"You are a senior researcher tasked with writing a cohesive report for a research query. \"\n",
                "    \"You will be provided with the original query, and some initial research done by a research assistant.\\n\"\n",
                "    \"You should first come up with an outline for the report that describes the structure and \"\n",
                "    \"flow of the report. Then, generate the report and return that as your final output.\\n\"\n",
                "    \"The final output should be in markdown format, and it should be lengthy and detailed. Aim \"\n",
                "    \"for 5-10 pages of content, at least 1000 words.\"\n",
                ")\n",
                "\n",
                "PUSH_INSTRUCTIONS = \"\"\"You are a member of a research team and will be provided with a short summary of a report.\n",
                "When you receive the report summary, you send a push notification to the user using your tool, informing them that research is complete,\n",
                "and including the report summary you receive\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6a2db7ee",
            "metadata": {},
            "source": [
                "#### SCHEMAS "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "1d139159",
            "metadata": {},
            "outputs": [],
            "source": [
                "class WebSearchItem(BaseModel):\n",
                "    reason: str = Field(description=\"Your reasoning for why this search is important to the query.\")\n",
                "    query: str = Field(description=\"The search term to use for the web search.\")\n",
                "\n",
                "class WebSearchPlan(BaseModel):\n",
                "    searches: List[WebSearchItem] = Field(description=\"A list of web searches to perform.\")\n",
                "\n",
                "class ReportData(BaseModel):\n",
                "    short_summary: str = Field(description=\"A short 2-3 sentence summary of the findings.\")\n",
                "    markdown_report: str = Field(description=\"The final report\")\n",
                "    follow_up_questions: List[str] = Field(description=\"Suggested topics to research further\")\n",
                "\n",
                "# --- 3. TOOLS ---\n",
                "@tool\n",
                "def web_search_tool(query: str) -> str:\n",
                "    \"\"\"Search the web for a given term. Use this for research.\"\"\"\n",
                "    try:\n",
                "        # Use Tavily for search (expects TAVILY_API_KEY in env)\n",
                "        search = TavilySearchResults(max_results=3)\n",
                "        # Tavily returns a list of dicts, so we convert to string\n",
                "        return str(search.invoke(query))\n",
                "    except Exception as e:\n",
                "        return f\"Error performing search: {e}\"\n",
                "\n",
                "@tool\n",
                "def push_notification_tool(message: str):\n",
                "    \"\"\"Send a push notification with this brief message\"\"\"\n",
                "    user_key = os.getenv(\"PUSHOVER_USER\")\n",
                "    api_token = os.getenv(\"PUSHOVER_TOKEN\")\n",
                "    if not user_key or not api_token:\n",
                "        return \"Error: PUSHOVER_USER or PUSHOVER_TOKEN not found in environment.\"\n",
                "        \n",
                "    payload = {\"user\": user_key, \"token\": api_token, \"message\": message}\n",
                "    pushover_url = \"https://api.pushover.net/1/messages.json\"\n",
                "    try:\n",
                "        response = requests.post(pushover_url, data=payload)\n",
                "        if response.status_code == 200:\n",
                "             return \"success\"\n",
                "        else:\n",
                "             return f\"Failed to send notification: {response.text}\"\n",
                "    except Exception as e:\n",
                "        return f\"Error sending notification: {e}\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2078b766",
            "metadata": {},
            "source": [
                "#### Graph State & Nodes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "920adea8",
            "metadata": {},
            "outputs": [],
            "source": [
                "#  STATE DEFINITION\n",
                "class ResearchState(TypedDict):\n",
                "    # messages tracks the conversation history\n",
                "    messages: Annotated[List[BaseMessage], add_messages]\n",
                "    # Specialized fields to hold intermediate data\n",
                "    query: str\n",
                "    search_plan: List[WebSearchItem]\n",
                "    search_results: List[str]\n",
                "    report: ReportData\n",
                "\n",
                "# NODE IMPLEMENTATIONS \n",
                "model_mini = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
                "model_large = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
                "\n",
                "async def planner_node(state: ResearchState):\n",
                "    \"\"\"PlannerAgent: Logic to generate the search plan.\"\"\"\n",
                "    planner = model_mini.with_structured_output(WebSearchPlan)\n",
                "    response = await planner.ainvoke([\n",
                "        SystemMessage(content=PLANNER_INSTRUCTIONS),\n",
                "        HumanMessage(content=f\"Query: {state['query']}\")\n",
                "    ])\n",
                "    return {\n",
                "        \"search_plan\": response.searches,\n",
                "        \"messages\": [AIMessage(content=f\"Planned {len(response.searches)} searches.\")]\n",
                "    }\n",
                "\n",
                "async def search_node(state: ResearchState):\n",
                "    \"\"\"SearchAgent: Executes processes with autonomous tool loops.\"\"\"\n",
                "    search_agent = model_mini.bind_tools([web_search_tool])\n",
                "    \n",
                "    async def perform_single_search(item: WebSearchItem):\n",
                "        # Agent ReAct Loop\n",
                "        initial_msg = [\n",
                "            SystemMessage(content=SEARCH_INSTRUCTIONS),\n",
                "            HumanMessage(content=f\"Search term: {item.query}\\nReason: {item.reason}\")\n",
                "        ]\n",
                "        # 1. Ask model\n",
                "        res1 = await search_agent.ainvoke(initial_msg)\n",
                "        messages = list(initial_msg) + [res1]\n",
                "        \n",
                "        # 2. Check and Execute Tool\n",
                "        if res1.tool_calls:\n",
                "            for tc in res1.tool_calls:\n",
                "                # In this simulated environment, we invoke the tool directly\n",
                "                if tc['name'] == 'web_search_tool':\n",
                "                    out = web_search_tool.invoke(tc['args'])\n",
                "                    messages.append(ToolMessage(content=str(out), tool_call_id=tc['id']))\n",
                "            \n",
                "            # 3. Get Summary from Model\n",
                "            res2 = await search_agent.ainvoke(messages)\n",
                "            return f\"Summary for {item.query}: {res2.content}\"\n",
                "        \n",
                "        return f\"Summary for {item.query}: {res1.content}\"\n",
                "\n",
                "    # Parallel execution\n",
                "    results = await asyncio.gather(*[perform_single_search(i) for i in state[\"search_plan\"]])\n",
                "    \n",
                "    return {\n",
                "        \"search_results\": results,\n",
                "        \"messages\": [AIMessage(content=\"Web research completed.\")]\n",
                "    }\n",
                "\n",
                "async def writer_node(state: ResearchState):\n",
                "    \"\"\"WriterAgent: Synthesizes the final report.\"\"\"\n",
                "    writer = model_large.with_structured_output(ReportData)\n",
                "    prompt = f\"Original query: {state['query']}\\n\\nResearch Results:\\n\" + \"\\n\".join(state[\"search_results\"])\n",
                "    \n",
                "    response = await writer.ainvoke([\n",
                "        SystemMessage(content=WRITER_INSTRUCTIONS),\n",
                "        HumanMessage(content=prompt)\n",
                "    ])\n",
                "    return {\n",
                "        \"report\": response,\n",
                "        \"messages\": [AIMessage(content=\"Final report generated.\")]\n",
                "    }\n",
                "\n",
                "async def push_node(state: ResearchState):\n",
                "    \"\"\"PushAgent: Autonomous push notification.\"\"\"\n",
                "    pusher = model_mini.bind_tools([push_notification_tool])\n",
                "    summary = state[\"report\"].short_summary\n",
                "    \n",
                "    messages = [\n",
                "        SystemMessage(content=PUSH_INSTRUCTIONS),\n",
                "        HumanMessage(content=summary)\n",
                "    ]\n",
                "    \n",
                "    res1 = await pusher.ainvoke(messages)\n",
                "    messages.append(res1)\n",
                "    \n",
                "    if res1.tool_calls:\n",
                "         for tc in res1.tool_calls:\n",
                "             if tc['name'] == 'push_notification_tool':\n",
                "                 out = push_notification_tool.invoke(tc['args'])\n",
                "                 messages.append(ToolMessage(content=str(out), tool_call_id=tc['id']))\n",
                "         \n",
                "         res2 = await pusher.ainvoke(messages)\n",
                "         return {\"messages\": [AIMessage(content=\"Notification pushed and confirmed.\")]}\n",
                "    \n",
                "    return {\"messages\": [AIMessage(content=\"Notification step completed (no call).\")]}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ffb41540",
            "metadata": {},
            "source": [
                "####  Assembly & Execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "355398fa",
            "metadata": {},
            "outputs": [],
            "source": [
                "# GRAPH ASSEMBLY \n",
                "builder = StateGraph(ResearchState)\n",
                "\n",
                "builder.add_node(\"planner\", planner_node)\n",
                "builder.add_node(\"researcher\", search_node)\n",
                "builder.add_node(\"writer\", writer_node)\n",
                "builder.add_node(\"notifier\", push_node)\n",
                "\n",
                "builder.add_edge(START, \"planner\")\n",
                "builder.add_edge(\"planner\", \"researcher\")\n",
                "builder.add_edge(\"researcher\", \"writer\")\n",
                "builder.add_edge(\"writer\", \"notifier\")\n",
                "builder.add_edge(\"notifier\", END)\n",
                "\n",
                "# Compile with a checkpointer for LangSmith thread-level tracing\n",
                "memory = MemorySaver()\n",
                "graph = builder.compile(checkpointer=memory)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "0a86fab5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Starting Research: What are the most popular and successful AI Agent frameworks in May 2025 ---\n",
                        "[HUMAN]: What are the most popular and successful AI Agent frameworks in May 2025...\n",
                        "[AI]: Planned 5 searches....\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\ADEGOKE\\AppData\\Local\\Temp\\ipykernel_17516\\2152884298.py:19: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
                        "  search = TavilySearchResults(max_results=3)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[AI]: Web research completed....\n"
                    ]
                },
                {
                    "ename": "BadRequestError",
                    "evalue": "Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=ReportData> {\"short_summary\": \"This report outlines the most popular and successful AI Agent frameworks as of May 2025, including their key features, strengths, and market share. The report also discusses the current market trends and future projections for the AI Agent framework market. The top agentic AI frameworks in 2025 are LangChain, LangGraph, AutoGen, AgentGPT, and AutoGPT, with LangChain and LangGraph being the most widely used. The market is expected to grow rapidly, with a projected market size of $7.38 billion by the end of 2025 and an estimated $52.62 billion by 2030.\", \"markdown_report\": \"# AI Agent Frameworks Report\\\\n\\\\n## Table of Contents\\\\n\\\\n1. Introduction\\\\n2. Popular AI Agent Frameworks\\\\n3. Successful AI Agent Frameworks\\\\n4. AI Agent Framework Ranking 2025\\\\n5. AI Agent Framework Comparison\\\\n6. AI Agent Framework Market Share 2025\\\\n7. Market Trends and Future Projections\\\\n\\\\n## Introduction\\\\n\\\\nThe AI Agent framework market is expected to grow rapidly, with a projected market size of $7.38 billion by the end of 2025 and an estimated $52.62 billion by 2030. This report aims to provide an overview of the most popular and successful AI Agent frameworks as of May 2025, including their key features, strengths, and market share.\\\\n\\\\n## Popular AI Agent Frameworks\\\\n\\\\nPopular AI Agent frameworks include:\\\\n\\\\n* Transformers Agents (Hugging Face): leverages transformer models for complex NLP tasks\\\\n* LangGraph: graph-based workflow for complex tasks with branching and error handling\\\\n* OpenAI Agents SDK: high-level toolchain with integrated tools and official support\\\\n* AutoGen: multi-agent conversation framework for complex collaborative tasks\\\\n* Hashbrown: browser-native agent framework\\\\n* Project Astra: universal AI assistant with multiple modalities (text, voice, images, video)\\\\n\\\\n## Successful AI Agent Frameworks\\\\n\\\\nThe most successful AI Agent frameworks include:\\\\n\\\\n* LangGraph: A graph-based solution that provides precise control and is ideal for complex tasks.\\\\n* AutoGen: A conversation-based solution that offers natural and flexible dialogues.\\\\n* CrewAI: A role-based orchestration framework that can tackle complex tasks through a \\\\\\\\\"cast\\\\\\\\\" of specialized agents.\\\\n* Smolagents: A minimal code-driven pattern framework that is ideal for simple tasks.\\\\n* Semantic Kernel: An enterprise-grade generative AI application framework that provides core abstractions for creating agents.\\\\n* LlamaIndex Agents: A retrieval-centric framework that shines for complex natural language tasks.\\\\n* Strands Agents SDK: A model-agnostic agent framework that runs anywhere and supports multiple model providers with reasoning and tool use.\\\\n\\\\n## AI Agent Framework Ranking 2025\\\\n\\\\nThe current market leaders in AI Agent frameworks as of May 2025 are:\\\\n\\\\n* LangChain: A robust solution for generative AI and NLP applications, allowing developers to build, test, and deploy AI agents capable of handling complex natural language tasks.\\\\n* LangGraph: A framework that simplifies the development of intelligent agents by integrating advanced machine learning (ML) models and making them accessible through a cohesive, user-friendly API.\\\\n* AutoGen: A growing framework that is gaining popularity, especially for enterprise use cases.\\\\n* AgentGPT and AutoGPT: Popular for experimentation and prototyping.\\\\n\\\\n## AI Agent Framework Comparison\\\\n\\\\nThe following AI Agent frameworks are compared:\\\\n\\\\n* LangGraph: A graph-based workflow of prompts that offers explicit DAG control, branching, and debugging. It is best for complex multi-step tasks with branching and advanced error handling.\\\\n* OpenAI Agents SDK: A high-level OpenAI toolchain that provides integrated tools such as web and file search. It is best for teams relying on OpenAI\\'s ecosystem who want official support and specialized features.\\\\n* Langflow: A visual no-code framework that is part of the n8n platform.\\\\n* n8n: A visual no-code platform that offers agentic workflows, not just agents.\\\\n* CrewAI: A role-playing AI agent framework that focuses on collaborative problem-solving and team dynamics. It is best for simulating complex organizational tasks.\\\\n* AutoGen: A programming-first framework that facilitates the creation of AI systems that can manage changing settings and enhance their overall performance over time.\\\\n* SmolAgents: A programming-first framework that is ideal for minimal code-driven patterns.\\\\n* Semantic Kernel: A framework positioned in the enterprise space that provides a robust solution for integrating LLMs with conventional programming languages.\\\\n* LlamaIndex Agents: A retrieval-centric framework that shines for efficient data indexing and retrieval.\\\\n\\\\n## AI Agent Framework Market Share 2025\\\\n\\\\nThe AI Agent framework market is expected to grow rapidly, with a projected market size of $7.38 billion by the end of 2025 and an estimated $52.62 billion by 2030. The solution segment is expected to dominate the market in 2025, with a 64.7% share, providing ready-to-deploy AI agents that meet specific business needs.\\\\n\\\\n## Market Trends and Future Projections\\\\n\\\\nThe market is expected to experience significant growth, with an estimated $52.62 billion by 2030, and a Compound Annual Growth Rate of 46.3%. Key trends in the market include:\\\\n\\\\n* Multi-modal agents: Frameworks will native support for voice, vision, and text in unified workflows.\\\\n* No-code agent builders: Visual development environments will democratize agent creation.\\\\n* Industry-specific frameworks: Specialized frameworks for healthcare, finance, and legal sectors will emerge, built on these foundational platforms.\\\\n\\\\n## Conclusion\\\\n\\\\nThis report provides an overview of the most popular and successful AI Agent frameworks as of May 2025, including their key features, strengths, and market share. The report also discusses the current market trends and future projections for the AI Agent framework market. The top agentic AI frameworks in 2025 are LangChain, LangGraph, AutoGen, AgentGPT, and AutoGPT, with LangChain and LangGraph being the most widely used. The market is expected to grow rapidly, with a projected market size of $7.38 billion by the end of 2025 and an estimated $52.62 billion by 2030.\", \"follow_up_questions\": \"What are the key differences between LangChain and LangGraph? How will the increasing adoption of no-code agent builders impact the market? What are the potential risks and challenges associated with the growing AI Agent framework market? How will industry-specific frameworks emerge and evolve?\"} </function>'}}",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Launch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_workflow(\u001b[33m\"\u001b[39m\u001b[33mWhat are the most popular and successful AI Agent frameworks in May 2025\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mrun_workflow\u001b[39m\u001b[34m(user_query)\u001b[39m\n\u001b[32m      6\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mworkshop_user_1\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# We stream the values to show progress in the workshop\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph.astream(inputs, config=config, stream_mode=\u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m event:\n\u001b[32m     11\u001b[39m         last_msg = event[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m]\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\agenticAI\\Lib\\site-packages\\langgraph\\pregel\\main.py:2971\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2969\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2970\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2972\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2973\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2974\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2975\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2976\u001b[39m ):\n\u001b[32m   2977\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2978\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2979\u001b[39m         stream_mode,\n\u001b[32m   2980\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2983\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2984\u001b[39m     ):\n\u001b[32m   2985\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\agenticAI\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\agenticAI\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\agenticAI\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\agenticAI\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mwriter_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     65\u001b[39m writer = model_large.with_structured_output(ReportData)\n\u001b[32m     66\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mResearch Results:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(state[\u001b[33m\"\u001b[39m\u001b[33msearch_results\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m writer.ainvoke([\n\u001b[32m     69\u001b[39m     SystemMessage(content=WRITER_INSTRUCTIONS),\n\u001b[32m     70\u001b[39m     HumanMessage(content=prompt)\n\u001b[32m     71\u001b[39m ])\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     73\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreport\u001b[39m\u001b[33m\"\u001b[39m: response,\n\u001b[32m     74\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [AIMessage(content=\u001b[33m\"\u001b[39m\u001b[33mFinal report generated.\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m     75\u001b[39m }\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\agenticAI\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3191\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3189\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3190\u001b[39m                 part = functools.partial(step.ainvoke, input_, config)\n\u001b[32m-> \u001b[39m\u001b[32m3191\u001b[39m             input_ = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(part(), context, create_task=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3192\u001b[39m     \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3193\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\agenticAI\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5570\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5563\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5564\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5565\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5568\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5569\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5570\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5571\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5572\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5573\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5574\u001b[39m     )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\agenticAI\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:425\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    417\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    422\u001b[39m     **kwargs: Any,\n\u001b[32m    423\u001b[39m ) -> AIMessage:\n\u001b[32m    424\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    426\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    427\u001b[39m         stop=stop,\n\u001b[32m    428\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    429\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    430\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    431\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    432\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    433\u001b[39m         **kwargs,\n\u001b[32m    434\u001b[39m     )\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    436\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m, cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n\u001b[32m    437\u001b[39m     )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\agenticAI\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1132\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1124\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1125\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1129\u001b[39m     **kwargs: Any,\n\u001b[32m   1130\u001b[39m ) -> LLMResult:\n\u001b[32m   1131\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1133\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1134\u001b[39m     )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\agenticAI\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1090\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m   1078\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1079\u001b[39m             *[\n\u001b[32m   1080\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1088\u001b[39m             ]\n\u001b[32m   1089\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m   1091\u001b[39m flattened_outputs = [\n\u001b[32m   1092\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1093\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m   1094\u001b[39m ]\n\u001b[32m   1095\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\agenticAI\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1343\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1341\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1342\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1344\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1345\u001b[39m     )\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1347\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\agenticAI\\Lib\\site-packages\\langchain_groq\\chat_models.py:614\u001b[39m, in \u001b[36mChatGroq._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    609\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    610\u001b[39m params = {\n\u001b[32m    611\u001b[39m     **params,\n\u001b[32m    612\u001b[39m     **kwargs,\n\u001b[32m    613\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client.create(messages=message_dicts, **params)\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, params)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\agenticAI\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:941\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    722\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    723\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    780\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    781\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m    782\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    783\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    784\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    939\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m    942\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/openai/v1/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    943\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m    944\u001b[39m             {\n\u001b[32m    945\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m    946\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m    947\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mcitation_options\u001b[39m\u001b[33m\"\u001b[39m: citation_options,\n\u001b[32m    948\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mcompound_custom\u001b[39m\u001b[33m\"\u001b[39m: compound_custom,\n\u001b[32m    949\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mdisable_tool_validation\u001b[39m\u001b[33m\"\u001b[39m: disable_tool_validation,\n\u001b[32m    950\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m: documents,\n\u001b[32m    951\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mexclude_domains\u001b[39m\u001b[33m\"\u001b[39m: exclude_domains,\n\u001b[32m    952\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m    953\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m    954\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m    955\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude_domains\u001b[39m\u001b[33m\"\u001b[39m: include_domains,\n\u001b[32m    956\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude_reasoning\u001b[39m\u001b[33m\"\u001b[39m: include_reasoning,\n\u001b[32m    957\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m    958\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m    959\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m    960\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m    961\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    962\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m    963\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m    964\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m    965\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m    966\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_format\u001b[39m\u001b[33m\"\u001b[39m: reasoning_format,\n\u001b[32m    967\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m    968\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msearch_settings\u001b[39m\u001b[33m\"\u001b[39m: search_settings,\n\u001b[32m    969\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m    970\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m    971\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m    972\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m    973\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m    974\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m    975\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m    976\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m    977\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m    978\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m    979\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m    980\u001b[39m             },\n\u001b[32m    981\u001b[39m             completion_create_params.CompletionCreateParams,\n\u001b[32m    982\u001b[39m         ),\n\u001b[32m    983\u001b[39m         options=make_request_options(\n\u001b[32m    984\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m    985\u001b[39m         ),\n\u001b[32m    986\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m    987\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    988\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m    989\u001b[39m     )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\agenticAI\\Lib\\site-packages\\groq\\_base_client.py:1762\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1748\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1750\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1757\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1758\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1759\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1760\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1761\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\agenticAI\\Lib\\site-packages\\groq\\_base_client.py:1576\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1573\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1575\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1576\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1578\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1580\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
                        "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=ReportData> {\"short_summary\": \"This report outlines the most popular and successful AI Agent frameworks as of May 2025, including their key features, strengths, and market share. The report also discusses the current market trends and future projections for the AI Agent framework market. The top agentic AI frameworks in 2025 are LangChain, LangGraph, AutoGen, AgentGPT, and AutoGPT, with LangChain and LangGraph being the most widely used. The market is expected to grow rapidly, with a projected market size of $7.38 billion by the end of 2025 and an estimated $52.62 billion by 2030.\", \"markdown_report\": \"# AI Agent Frameworks Report\\\\n\\\\n## Table of Contents\\\\n\\\\n1. Introduction\\\\n2. Popular AI Agent Frameworks\\\\n3. Successful AI Agent Frameworks\\\\n4. AI Agent Framework Ranking 2025\\\\n5. AI Agent Framework Comparison\\\\n6. AI Agent Framework Market Share 2025\\\\n7. Market Trends and Future Projections\\\\n\\\\n## Introduction\\\\n\\\\nThe AI Agent framework market is expected to grow rapidly, with a projected market size of $7.38 billion by the end of 2025 and an estimated $52.62 billion by 2030. This report aims to provide an overview of the most popular and successful AI Agent frameworks as of May 2025, including their key features, strengths, and market share.\\\\n\\\\n## Popular AI Agent Frameworks\\\\n\\\\nPopular AI Agent frameworks include:\\\\n\\\\n* Transformers Agents (Hugging Face): leverages transformer models for complex NLP tasks\\\\n* LangGraph: graph-based workflow for complex tasks with branching and error handling\\\\n* OpenAI Agents SDK: high-level toolchain with integrated tools and official support\\\\n* AutoGen: multi-agent conversation framework for complex collaborative tasks\\\\n* Hashbrown: browser-native agent framework\\\\n* Project Astra: universal AI assistant with multiple modalities (text, voice, images, video)\\\\n\\\\n## Successful AI Agent Frameworks\\\\n\\\\nThe most successful AI Agent frameworks include:\\\\n\\\\n* LangGraph: A graph-based solution that provides precise control and is ideal for complex tasks.\\\\n* AutoGen: A conversation-based solution that offers natural and flexible dialogues.\\\\n* CrewAI: A role-based orchestration framework that can tackle complex tasks through a \\\\\\\\\"cast\\\\\\\\\" of specialized agents.\\\\n* Smolagents: A minimal code-driven pattern framework that is ideal for simple tasks.\\\\n* Semantic Kernel: An enterprise-grade generative AI application framework that provides core abstractions for creating agents.\\\\n* LlamaIndex Agents: A retrieval-centric framework that shines for complex natural language tasks.\\\\n* Strands Agents SDK: A model-agnostic agent framework that runs anywhere and supports multiple model providers with reasoning and tool use.\\\\n\\\\n## AI Agent Framework Ranking 2025\\\\n\\\\nThe current market leaders in AI Agent frameworks as of May 2025 are:\\\\n\\\\n* LangChain: A robust solution for generative AI and NLP applications, allowing developers to build, test, and deploy AI agents capable of handling complex natural language tasks.\\\\n* LangGraph: A framework that simplifies the development of intelligent agents by integrating advanced machine learning (ML) models and making them accessible through a cohesive, user-friendly API.\\\\n* AutoGen: A growing framework that is gaining popularity, especially for enterprise use cases.\\\\n* AgentGPT and AutoGPT: Popular for experimentation and prototyping.\\\\n\\\\n## AI Agent Framework Comparison\\\\n\\\\nThe following AI Agent frameworks are compared:\\\\n\\\\n* LangGraph: A graph-based workflow of prompts that offers explicit DAG control, branching, and debugging. It is best for complex multi-step tasks with branching and advanced error handling.\\\\n* OpenAI Agents SDK: A high-level OpenAI toolchain that provides integrated tools such as web and file search. It is best for teams relying on OpenAI\\'s ecosystem who want official support and specialized features.\\\\n* Langflow: A visual no-code framework that is part of the n8n platform.\\\\n* n8n: A visual no-code platform that offers agentic workflows, not just agents.\\\\n* CrewAI: A role-playing AI agent framework that focuses on collaborative problem-solving and team dynamics. It is best for simulating complex organizational tasks.\\\\n* AutoGen: A programming-first framework that facilitates the creation of AI systems that can manage changing settings and enhance their overall performance over time.\\\\n* SmolAgents: A programming-first framework that is ideal for minimal code-driven patterns.\\\\n* Semantic Kernel: A framework positioned in the enterprise space that provides a robust solution for integrating LLMs with conventional programming languages.\\\\n* LlamaIndex Agents: A retrieval-centric framework that shines for efficient data indexing and retrieval.\\\\n\\\\n## AI Agent Framework Market Share 2025\\\\n\\\\nThe AI Agent framework market is expected to grow rapidly, with a projected market size of $7.38 billion by the end of 2025 and an estimated $52.62 billion by 2030. The solution segment is expected to dominate the market in 2025, with a 64.7% share, providing ready-to-deploy AI agents that meet specific business needs.\\\\n\\\\n## Market Trends and Future Projections\\\\n\\\\nThe market is expected to experience significant growth, with an estimated $52.62 billion by 2030, and a Compound Annual Growth Rate of 46.3%. Key trends in the market include:\\\\n\\\\n* Multi-modal agents: Frameworks will native support for voice, vision, and text in unified workflows.\\\\n* No-code agent builders: Visual development environments will democratize agent creation.\\\\n* Industry-specific frameworks: Specialized frameworks for healthcare, finance, and legal sectors will emerge, built on these foundational platforms.\\\\n\\\\n## Conclusion\\\\n\\\\nThis report provides an overview of the most popular and successful AI Agent frameworks as of May 2025, including their key features, strengths, and market share. The report also discusses the current market trends and future projections for the AI Agent framework market. The top agentic AI frameworks in 2025 are LangChain, LangGraph, AutoGen, AgentGPT, and AutoGPT, with LangChain and LangGraph being the most widely used. The market is expected to grow rapidly, with a projected market size of $7.38 billion by the end of 2025 and an estimated $52.62 billion by 2030.\", \"follow_up_questions\": \"What are the key differences between LangChain and LangGraph? How will the increasing adoption of no-code agent builders impact the market? What are the potential risks and challenges associated with the growing AI Agent framework market? How will industry-specific frameworks emerge and evolve?\"} </function>'}}",
                        "During task with name 'writer' and id '96b6f444-f080-16cf-e95a-8c24c3465bdc'"
                    ]
                }
            ],
            "source": [
                "# 7. RUNTIME EXECUTION\n",
                "async def run_workflow(user_query: str):\n",
                "    print(f\"--- Starting Research: {user_query} ---\")\n",
                "    \n",
                "    inputs = {\"query\": user_query, \"messages\": [HumanMessage(content=user_query)]}\n",
                "    config = {\"configurable\": {\"thread_id\": \"workshop_user_1\"}}\n",
                "    \n",
                "    # We stream the values to show progress in the workshop\n",
                "    async for event in graph.astream(inputs, config=config, stream_mode=\"values\"):\n",
                "        if \"messages\" in event:\n",
                "            last_msg = event[\"messages\"][-1]\n",
                "            print(f\"[{last_msg.type.upper()}]: {last_msg.content[:100]}...\")\n",
                "            if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:\n",
                "                print(f\"  [TOOL CALL]: {last_msg.tool_calls[0]['name']}\")\n",
                "\n",
                "    # Final Output Rendering\n",
                "    final_state = await graph.aget_state(config)\n",
                "    report = final_state.values[\"report\"]\n",
                "    \n",
                "    display(Markdown(\"# Final Research Report\"))\n",
                "    display(Markdown(report.markdown_report))\n",
                "    print(\"\\nFollow-up Questions:\")\n",
                "    for q in report.follow_up_questions:\n",
                "        print(f\"- {q}\")\n",
                "\n",
                "# Launch\n",
                "await run_workflow(\"What are the most popular and successful AI Agent frameworks in May 2025\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "agenticAI",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        },
        "nbformat": 4,
        "nbformat_minor": 5
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
