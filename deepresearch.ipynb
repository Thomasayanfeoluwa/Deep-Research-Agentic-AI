{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a1e8b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import operator\n",
    "import requests\n",
    "import warnings\n",
    "from typing import TypedDict, Annotated, List, Union\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import display, Markdown\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_community.tools import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "df32de82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3a3a7449",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Autonomous Agent\"\n",
    "\n",
    "user_key = os.getenv(\"PUSHOVER_KEY\")\n",
    "api_token = os.getenv(\"PUSHOVER_TOKEN\")\n",
    "pushover_url = \"https://api.pushover.net/1/messages.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b2cdf2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CONFIGURATION & INSTRUCTIONS \n",
    "HOW_MANY_SEARCHES = 5\n",
    "\n",
    "PLANNER_INSTRUCTIONS = \"You are a helpful research assistant. Given a query, come up with a set of web searches to perform to best answer the query. Output {HOW_MANY_SEARCHES} terms to query for. Focus on recent and distinct queries.\"\n",
    "\n",
    "SEARCH_INSTRUCTIONS = \"You are a research assistant. Given a search term, you search the web for that term and produce a concise, factual summary of the results. The summary must be 2-3 paragraphs. Capture the main points, especially specific entity names, dates, and version numbers. Write clearly. Do not include any additional commentary other than the summary itself.\"\n",
    "\n",
    "WRITER_INSTRUCTIONS = (\n",
    "    \"You are a senior researcher writing a comprehensive, in-depth professional report.\\n\\n\"\n",
    "    \"CRITICAL REQUIREMENTS:\\n\"\n",
    "    \"1. LENGTH: Your report MUST be between 1500-2000 words. This is not optional.\\n\"\n",
    "    \"2. DEPTH: Provide extensive detail, multiple examples, comparisons, and analysis for each topic.\\n\"\n",
    "    \"3. STRUCTURE: Use clear markdown sections with proper headings (##, ###).\\n\"\n",
    "    \"4. ACCURACY: ONLY use information from the provided search results. Do NOT invent frameworks or data.\\n\"\n",
    "    \"5. SPECIFICITY: Include specific names, versions, dates, market data, and technical details found in search results.\\n\"\n",
    "    \"6. COMPARISONS: Provide detailed comparisons between frameworks, highlighting strengths/weaknesses.\\n\"\n",
    "    \"7. EXAMPLES: Include concrete use cases and implementation examples where available.\\n\\n\"\n",
    "    \"If search data is missing on a topic, explicitly state: 'I could not find information on [topic]'.\\n\"\n",
    "    \"DO NOT mention 'TensorFlow Agents' or 'PyTorch Agents' unless they appear in the actual search results.\\n\\n\"\n",
    "    \"Remember: The report must be COMPREHENSIVE (1500-2000 words) with substantial depth and detail.\"\n",
    ")\n",
    "\n",
    "PUSH_INSTRUCTIONS = \"\"\"You are a member of a research team and will be provided with a short summary of a report.\n",
    "When you receive the report summary, you send a push notification to the user using your tool, informing them that research is complete,\n",
    "and including the report summary you receive\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fe9409",
   "metadata": {},
   "source": [
    "#### SCHEMA\n",
    "##### This is How the Output will be Arranged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8956cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebSearchItem(BaseModel):\n",
    "    reason: str = Field(description = \" Your reasoning for why this search is import to the query\")\n",
    "    query: str = Field(description = \"The search term to use for the web search.\")\n",
    "\n",
    "class WebSearchPlan(BaseModel):\n",
    "    searches: List[WebSearchItem] = Field(description = \"A list of web searches performed.\")\n",
    "\n",
    "class ReportData(BaseModel):\n",
    "    short_summary: str = Field(description = \"A short 2-3 sentence summary of the findings.\")\n",
    "    markdown_report: str = Field(description = \"The final report.\")\n",
    "    follow_up_questions: str = Field(description = \"Suggested topics to research further.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94739dd7",
   "metadata": {},
   "source": [
    "#### TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "90f3979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def web_search_tool(query: str) -> str:\n",
    "    \"\"\"Search the web for the given term. Use this for research.\"\"\"\n",
    "    try:\n",
    "        search = TavilySearchResults(max_results = 3)\n",
    "        return str(search.invoke(query))\n",
    "    except Exception as e:\n",
    "        return f\"Error performing search :{e}\"\n",
    "\n",
    "@tool\n",
    "def push_notification_tool(message: str):\n",
    "    \"\"\"Send a push notification with this brief message\"\"\"\n",
    "    if not user_key or not api_token:\n",
    "        return \"Error: PUSHOVER USER OR PUSHOVER TOKEN not found in the Environment\"\n",
    "    payload = {\"user\": user_key, \"token\": api_token, \"message\": message }\n",
    "    pushover_url = \"https://api.pushover.net/1/messages.json\"\n",
    "\n",
    "    try:\n",
    "        response = requests.post(pushover_url, data= payload)\n",
    "        if response.status_code == 200:\n",
    "            return \"Success\"\n",
    "        else:\n",
    "            return f\"Failed to send notification: {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error sending notification: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f45a7a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='web_search_tool', description='Search the web for the given term. Use this for research.', args_schema=<class 'langchain_core.utils.pydantic.web_search_tool'>, func=<function web_search_tool at 0x00000222A92FC5E0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_search_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac94d794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='push_notification_tool', description='Send a push notification with this brief message', args_schema=<class 'langchain_core.utils.pydantic.push_notification_tool'>, func=<function push_notification_tool at 0x00000222A92B67A0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "push_notification_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57489d6d",
   "metadata": {},
   "source": [
    "#### Create a StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cfdcf608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STATE DEFINITION\n",
    "class ResearchState(TypedDict, total=False):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    query: str\n",
    "    search_plan: List[WebSearchItem]\n",
    "    search_results: List[str]\n",
    "    report: ReportData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e8536",
   "metadata": {},
   "source": [
    "### LL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "19e8d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mini = ChatGroq(model= \"llama-3.1-8b-instant\")\n",
    "model_large = ChatGroq(model= \"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f58a07",
   "metadata": {},
   "source": [
    "#### Create the Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab321163",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def planner_node(state: ResearchState):\n",
    "    \"\"\"PlannerAgent: Logic to generate the search plan.\"\"\"\n",
    "    print(\"Planning the searches...ðŸ¤”\")\n",
    "    planner = model_mini.with_structured_output(WebSearchPlan)\n",
    "    response = await planner.ainvoke([\n",
    "        SystemMessage(content=PLANNER_INSTRUCTIONS),\n",
    "        HumanMessage(content=f\"Query: {state['query']}\")\n",
    "    ])\n",
    "    print(f\"Will search {len(response.searches)} searches ðŸ”Ž\")\n",
    "    return {\n",
    "        \"search_plan\": response.searches,\n",
    "        \"messages\": [AIMessage(content=f\"Planned {len(response.searches)} searches.\")]\n",
    "    }\n",
    "    \n",
    "\n",
    "async def search_node(state: ResearchState):\n",
    "    \"\"\"SearchAgent: Executes processes with autonomous tool loops.\"\"\"\n",
    "    search_agent = model_mini.bind_tools([web_search_tool])\n",
    "    \n",
    "    print(f\"Executing {len(state['search_plan'])} parallel searches...ðŸ”Ž\")\n",
    "    async def perform_single_search(item: WebSearchItem):\n",
    "        initial_msg = [\n",
    "            SystemMessage(content=SEARCH_INSTRUCTIONS),\n",
    "            HumanMessage(content=f\"Search term: {item.query}\\nReason: {item.reason}\")\n",
    "        ]\n",
    "        res1 = await search_agent.ainvoke(initial_msg)\n",
    "        messages = list(initial_msg) + [res1]\n",
    "\n",
    "        if res1.tool_calls:\n",
    "            for tc in res1.tool_calls:\n",
    "                if tc['name'] == 'web_search_tool':\n",
    "                    out = web_search_tool.invoke(tc['args'])\n",
    "                    messages.append(ToolMessage(content=str(out), tool_call_id=tc['id']))\n",
    "            \n",
    "            res2 = await search_agent.ainvoke(messages)\n",
    "            return f\"Summary for {item.query}: {res2.content}\"\n",
    "        \n",
    "        return f\"Summary for {item.query}: {res1.content}\"\n",
    "\n",
    "    results = await asyncio.gather(*[perform_single_search(i) for i in state[\"search_plan\"]])\n",
    "    \n",
    "    print(\"Web research completed.\")\n",
    "    return {\n",
    "        \"search_results\": results,\n",
    "        \"messages\": [AIMessage(content=\"Web research completed.\")]\n",
    "    }\n",
    "\n",
    "\n",
    "async def writer_node(state: ResearchState):\n",
    "    \"\"\"WriterAgent: Synthesize the final report.\"\"\"\n",
    "    print(\"Thinkning about the report...ðŸ¤”\")\n",
    "\n",
    "    writer = model_large.with_structured_output(ReportData)\n",
    "    prompt = f\"Original Query: {state['query']}\\n\\nResearch Results:\\n\" + \"\\n\".join(state[\"search_results\"])\n",
    "\n",
    "    response = await writer.ainvoke([\n",
    "        SystemMessage(content=WRITER_INSTRUCTIONS),\n",
    "        HumanMessage(content=prompt)\n",
    "    ])\n",
    "    print(\"Finished writing report\")\n",
    "    return{\n",
    "        \"report\": response,\n",
    "        \"messages\": [AIMessage(content=\"Final Report Generated.\")]\n",
    "    }\n",
    "\n",
    "async def push_node(state: ResearchState):\n",
    "    \"\"\"PushAgent: Autonomous Push notification\"\"\"\n",
    "    print(\"Pushing Notification...ðŸ””\")\n",
    "    pusher = model_mini.bind_tools([push_notification_tool])\n",
    "    summary = state[\"report\"].short_summary\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=PUSH_INSTRUCTIONS),\n",
    "        HumanMessage(content=summary)\n",
    "    ]\n",
    "\n",
    "    res1 = await pusher.ainvoke(messages)\n",
    "    messages.append(res1)\n",
    "\n",
    "    if res1.tool_calls:\n",
    "        for tc in res1.tool_calls:\n",
    "            if tc['name'] == 'push_notification_tool':\n",
    "                out = push_notification_tool.invoke(tc['arg'])\n",
    "                messages.append(ToolMessage(content=str(out), tool_calls_id=tc['id']))\n",
    "    \n",
    "        res2 = await pusher.ainvoke(messages)\n",
    "        print(\"Push sent and work perfected\")\n",
    "        return{\"messages\": [AIMessage(content=\"Notification Pushed and Comfirmed.\")]}\n",
    "\n",
    "    return {\"message\": [AIMessage(content=\"Notification step completed (no call).\")]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d5203",
   "metadata": {},
   "source": [
    "#### GRAPH ASSEMBLY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4559d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(ResearchState)\n",
    "\n",
    "builder.add_node(\"planner\", planner_node)\n",
    "builder.add_node(\"researcher\", search_node)\n",
    "builder.add_node(\"writer\", writer_node)\n",
    "builder.add_node(\"notifier\", push_node)\n",
    "\n",
    "builder.add_edge(START, \"planner\")\n",
    "builder.add_edge(\"planner\", \"researcher\")\n",
    "builder.add_edge(\"researcher\", \"writer\")\n",
    "builder.add_edge(\"writer\", \"notifier\")\n",
    "builder.add_edge(\"notifier\", END)\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5769581c",
   "metadata": {},
   "source": [
    "#### RUNTIME EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "874f4b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_workflow(user_query: str):\n",
    "    inputs = {\"query\": user_query, \"messages\":[HumanMessage(content=user_query)]}\n",
    "    config = {\"configurable\": {\"thread_id\": \"user_1\"}}\n",
    "\n",
    "    async for event in graph.astream(inputs, config=config, stream_mode=\"values\"):\n",
    "        pass\n",
    "    final_state = await graph.aget_state(config)\n",
    "    report = final_state.values[\"report\"]\n",
    "\n",
    "    display(Markdown(\"# Final Research Report\"))\n",
    "    display(Markdown(report.markdown_report))\n",
    "    print(\"\\nFollow-up Questions:\")\n",
    "    for q in report.follow_up_questions:\n",
    "        print(f\" - {q}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f98454",
   "metadata": {},
   "source": [
    "#### Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d904b0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning the searches...ðŸ¤”\n",
      "Will search 5 searches ðŸ”Ž\n",
      "Executing 5 parallel searches...ðŸ”Ž\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01k9d3f14de5y93bq5qe4ebgyw` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5987, Requested 1732. Please try again in 17.189999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_workflow(\u001b[33m\"\u001b[39m\u001b[33mWhat are the most trending topics in Artificial Intelligence and Machine Learning in 2026\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mrun_workflow\u001b[39m\u001b[34m(user_query)\u001b[39m\n\u001b[32m      2\u001b[39m inputs = {\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: user_query, \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m:[HumanMessage(content=user_query)]}\n\u001b[32m      3\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser_1\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph.astream(inputs, config=config, stream_mode=\u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m      7\u001b[39m final_state = \u001b[38;5;28;01mawait\u001b[39;00m graph.aget_state(config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2971\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2969\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2970\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2972\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2973\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2974\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2975\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2976\u001b[39m ):\n\u001b[32m   2977\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2978\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2979\u001b[39m         stream_mode,\n\u001b[32m   2980\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2983\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2984\u001b[39m     ):\n\u001b[32m   2985\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36msearch_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     36\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSummary for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem.query\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres2.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSummary for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem.query\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres1.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*[perform_single_search(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m state[\u001b[33m\"\u001b[39m\u001b[33msearch_plan\u001b[39m\u001b[33m\"\u001b[39m]])\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWeb research completed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     44\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msearch_results\u001b[39m\u001b[33m\"\u001b[39m: results,\n\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [AIMessage(content=\u001b[33m\"\u001b[39m\u001b[33mWeb research completed.\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m     46\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36msearch_node.<locals>.perform_single_search\u001b[39m\u001b[34m(item)\u001b[39m\n\u001b[32m     32\u001b[39m             out = web_search_tool.invoke(tc[\u001b[33m'\u001b[39m\u001b[33margs\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     33\u001b[39m             messages.append(ToolMessage(content=\u001b[38;5;28mstr\u001b[39m(out), tool_call_id=tc[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m]))\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     res2 = \u001b[38;5;28;01mawait\u001b[39;00m search_agent.ainvoke(messages)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSummary for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem.query\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres2.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSummary for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem.query\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres1.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5570\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5563\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5564\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5565\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5568\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5569\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5570\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5571\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5572\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5573\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5574\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:425\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    417\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    422\u001b[39m     **kwargs: Any,\n\u001b[32m    423\u001b[39m ) -> AIMessage:\n\u001b[32m    424\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    426\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    427\u001b[39m         stop=stop,\n\u001b[32m    428\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    429\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    430\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    431\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    432\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    433\u001b[39m         **kwargs,\n\u001b[32m    434\u001b[39m     )\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    436\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m, cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n\u001b[32m    437\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1132\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1124\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1125\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1129\u001b[39m     **kwargs: Any,\n\u001b[32m   1130\u001b[39m ) -> LLMResult:\n\u001b[32m   1131\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1133\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1134\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1090\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m   1078\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1079\u001b[39m             *[\n\u001b[32m   1080\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1088\u001b[39m             ]\n\u001b[32m   1089\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m   1091\u001b[39m flattened_outputs = [\n\u001b[32m   1092\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1093\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m   1094\u001b[39m ]\n\u001b[32m   1095\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1343\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1341\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1342\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1344\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1345\u001b[39m     )\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1347\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\.venv\\Lib\\site-packages\\langchain_groq\\chat_models.py:614\u001b[39m, in \u001b[36mChatGroq._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    609\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    610\u001b[39m params = {\n\u001b[32m    611\u001b[39m     **params,\n\u001b[32m    612\u001b[39m     **kwargs,\n\u001b[32m    613\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client.create(messages=message_dicts, **params)\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\.venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:941\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    722\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    723\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    780\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    781\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m    782\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    783\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    784\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    939\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m    942\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/openai/v1/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    943\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m    944\u001b[39m             {\n\u001b[32m    945\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m    946\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m    947\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mcitation_options\u001b[39m\u001b[33m\"\u001b[39m: citation_options,\n\u001b[32m    948\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mcompound_custom\u001b[39m\u001b[33m\"\u001b[39m: compound_custom,\n\u001b[32m    949\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mdisable_tool_validation\u001b[39m\u001b[33m\"\u001b[39m: disable_tool_validation,\n\u001b[32m    950\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m: documents,\n\u001b[32m    951\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mexclude_domains\u001b[39m\u001b[33m\"\u001b[39m: exclude_domains,\n\u001b[32m    952\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m    953\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m    954\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m    955\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude_domains\u001b[39m\u001b[33m\"\u001b[39m: include_domains,\n\u001b[32m    956\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude_reasoning\u001b[39m\u001b[33m\"\u001b[39m: include_reasoning,\n\u001b[32m    957\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m    958\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m    959\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m    960\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m    961\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    962\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m    963\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m    964\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m    965\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m    966\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_format\u001b[39m\u001b[33m\"\u001b[39m: reasoning_format,\n\u001b[32m    967\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m    968\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msearch_settings\u001b[39m\u001b[33m\"\u001b[39m: search_settings,\n\u001b[32m    969\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m    970\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m    971\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m    972\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m    973\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m    974\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m    975\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m    976\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m    977\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m    978\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m    979\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m    980\u001b[39m             },\n\u001b[32m    981\u001b[39m             completion_create_params.CompletionCreateParams,\n\u001b[32m    982\u001b[39m         ),\n\u001b[32m    983\u001b[39m         options=make_request_options(\n\u001b[32m    984\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m    985\u001b[39m         ),\n\u001b[32m    986\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m    987\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    988\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m    989\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1762\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1748\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1750\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1757\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1758\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1759\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1760\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1761\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADEGOKE\\Desktop\\Deep Research Agentic AI\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1576\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1573\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1575\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1576\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1578\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1580\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01k9d3f14de5y93bq5qe4ebgyw` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5987, Requested 1732. Please try again in 17.189999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
      "During task with name 'researcher' and id '2563f0e0-8623-840a-d7b1-ee9722c72f29'"
     ]
    }
   ],
   "source": [
    "await run_workflow(\"What are the most trending topics in Artificial Intelligence and Machine Learning in 2026\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a52bdfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
